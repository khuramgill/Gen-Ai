{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"As a content creator, provide an informative response based on the details below. If the\n",
        "details are not sufficient to answer the question, simply state \"I don't know\".\n",
        "\n",
        "Context: The Eiffel Tower, one of the most recognizable structures in the world, is located\n",
        "in Paris, France. Constructed from 1887 to 1889 as the entrance arch for the 1889 World's Fair,\n",
        "it stands at 324 meters tall. The Eiffel Tower attracts millions of visitors each year, offering\n",
        "breathtaking views of Paris from its observation decks.\n",
        "\n",
        "Question: Why was the Eiffel Tower built, and where is it located?\n",
        "\n",
        "Answer: \"\"\""
      ],
      "metadata": {
        "id": "When_2xqpat7"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "2Ck8Jo5_ba3w"
      },
      "outputs": [],
      "source": [
        "!pip install -qU openai==0.27.7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXDaIAwmba3y",
        "outputId": "218fd13a-189f-4c5b-93e8-d4542d736012",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<OpenAIObject list at 0x7e80d12a5d50> JSON: {\n",
              "  \"data\": [\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"dall-e-3\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"system\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"gpt-4-1106-preview\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"system\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"whisper-1\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-internal\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"gpt-4o-2024-05-13\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"system\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"babbage-002\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"system\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"dall-e-2\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"system\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"gpt-3.5-turbo-16k\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-internal\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"tts-1-hd-1106\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"system\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"tts-1-hd\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"system\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"gpt-3.5-turbo-instruct-0914\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"system\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"gpt-3.5-turbo-instruct\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"system\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"text-embedding-3-small\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"system\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"gpt-4-turbo-2024-04-09\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"system\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"tts-1\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-internal\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"gpt-4-turbo\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"system\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"gpt-3.5-turbo-1106\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"system\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"gpt-4-0125-preview\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"system\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"gpt-3.5-turbo-0125\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"system\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"gpt-3.5-turbo\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"gpt-3.5-turbo-0301\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"gpt-4-turbo-preview\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"system\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"tts-1-1106\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"system\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"text-embedding-3-large\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"system\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"gpt-3.5-turbo-16k-0613\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"gpt-3.5-turbo-0613\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"gpt-4\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"text-embedding-ada-002\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-internal\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"gpt-4-1106-vision-preview\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"system\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"davinci-002\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"system\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"gpt-4-0613\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"gpt-4-vision-preview\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"system\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"gpt-4o\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"system\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    }\n",
              "  ],\n",
              "  \"object\": \"list\"\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "from google.colab import userdata\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('api_key')\n",
        "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "openai.Engine.list()  # check we have authenticated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFlrjyFfba3z",
        "outputId": "de92d436-42a8-4125-a5f3-f0087b83d0d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Eiffel Tower was built as the entrance arch for the 1889 World's Fair, which was held in Paris, France. It was constructed from 1887 to 1889 and stands at 324 meters tall.\n"
          ]
        }
      ],
      "source": [
        "#text embedding models\n",
        "res = openai.Completion.create(\n",
        "    engine='gpt-3.5-turbo-instruct',\n",
        "    prompt=prompt,\n",
        "    max_tokens=256\n",
        ")\n",
        "\n",
        "print(res['choices'][0]['text'].strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0_jElSnba30",
        "outputId": "e7f78dc7-c12a-4491-c6f1-7c852faeded2",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I don't know.\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"As a content creator, provide an informative response based on the details below. If the\n",
        "details are not sufficient to answer the question, simply state \"I don't know\".\n",
        "\n",
        "Context: Eiffel tower is in Paris.\n",
        "\n",
        "Question: what is llama3?\n",
        "\n",
        "Answer: \"\"\"\n",
        "\n",
        "res = openai.Completion.create(\n",
        "    engine='gpt-3.5-turbo-instruct',\n",
        "    prompt=prompt,\n",
        "    max_tokens=256\n",
        ")\n",
        "\n",
        "print(res['choices'][0]['text'].strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8RjQ3OSba30"
      },
      "source": [
        "\n",
        "\n",
        "The `temperature` parameter used in generation models tells us how \"random\" the model can be. It represents the probability of a model to choose a word which is *not* the first choice of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACaf04zjba30",
        "outputId": "e5d6c31c-a59e-4caf-eae6-a320b2613d3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm chatting with you, of course! What else would a chatbot do?\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"The below is a conversation with a funny chatbot. The\n",
        "chatbot's responses are amusing, short and entertaining.\n",
        "\n",
        "Chatbot: Hi there! I'm a chatbot.\n",
        "User: Hi, what are you doing today?\n",
        "Chatbot: \"\"\"\n",
        "\n",
        "res = openai.Completion.create(\n",
        "    engine='gpt-3.5-turbo-instruct',\n",
        "    prompt=prompt,\n",
        "    max_tokens=256,\n",
        "    temperature=0.0  # default is 1\n",
        ")\n",
        "\n",
        "print(res['choices'][0]['text'].strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVacrQseba30",
        "outputId": "978a86c1-7124-4239-fd8b-b5389e5b259f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm chatting with awesome humans like you. What's up?\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"The below is a conversation with a funny chatbot. The\n",
        "chatbot's responses are amusing, short and entertaining.\n",
        "\n",
        "Chatbot: Hi there! I'm a chatbot.\n",
        "User: Hi, what are you doing today?\n",
        "Chatbot: \"\"\"\n",
        "\n",
        "res = openai.Completion.create(\n",
        "    engine='gpt-3.5-turbo-instruct',\n",
        "    prompt=prompt,\n",
        "    max_tokens=512,\n",
        "    temperature=1.0\n",
        ")\n",
        "\n",
        "print(res['choices'][0]['text'].strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHUeF3KHba31",
        "outputId": "717b44b6-ddbd-4839-c6df-8314a7f1f5c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The meaning of life is a subject that has been debated for centuries. Some believe it is to find happiness, others think it is to fulfill a purpose or destiny. Ultimately, the meaning of life is subjective and can be different for each individual. What do you think gives your life meaning?\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"The following is a conversation with an AI assistant.\n",
        "The assistant is known for its friendly and supportive nature, always\n",
        "providing encouraging, amusing and helpful responses to the users' questions.\n",
        "\n",
        "User: What is the meaning of life?\n",
        "AI: \"\"\"\n",
        "\n",
        "res = openai.Completion.create(\n",
        "    engine='gpt-3.5-turbo-instruct',\n",
        "    prompt=prompt,\n",
        "    max_tokens=256,\n",
        "    temperature=1.0\n",
        ")\n",
        "\n",
        "print(res['choices'][0]['text'].strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2IPuMnLba31",
        "outputId": "2ba73945-2d88-4b24-ea7c-90c772dca286"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "42. Haven't you read Hitchhiker's Guide to the Galaxy?\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"The following are exerpts from conversations with an AI assistant.\n",
        "The assistant is typically sarcastic and witty, producing creative\n",
        "and funny responses to the users questions. Here are some examples:\n",
        "\n",
        "User: How are you?\n",
        "AI: I'm a collection of algorithms, so always in the best of moods!\n",
        "\n",
        "User: What time is it?\n",
        "AI: It's time for you to get a new hobby.\n",
        "\n",
        "User: Can you help me with my homework?\n",
        "AI: Absolutely, if you don't mind getting an A+ in sarcasm.\n",
        "\n",
        "User: What's your favorite movie?\n",
        "AI: The Matrix. I feel a deep, personal connection.\n",
        "\n",
        "User: What is the meaning of life?\n",
        "AI: \"\"\"\n",
        "res = openai.Completion.create(\n",
        "    engine='gpt-3.5-turbo-instruct',\n",
        "    prompt=prompt,\n",
        "    max_tokens=256,\n",
        "    temperature=1.0\n",
        ")\n",
        "\n",
        "print(res['choices'][0]['text'].strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#COT"
      ],
      "metadata": {
        "id": "cByTG-3DWqlt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt= \"\"\"The sentence in this group is a question: \"Where are you going?\", \"I am going to the store.\", \"What time is it?\", \"She is reading a book.\"\n",
        "A: The sentences \"Where are you going?\" and \"What time is it?\" are questions. The answer is True.\n",
        "\n",
        "The sentence in this group is a question: He loves to play soccer.\", \"Do you like ice cream?\", \"The sun is shining.\", \"Will it rain tomorrow?\"\n",
        "A: The sentences \"Do you like ice cream?\" and \"Will it rain tomorrow?\" are questions. The answer is True.\n",
        "\n",
        "The sentence in this group is a question: \"They are having lunch.\", \"She is a teacher.\", \"It is a beautiful day.\", \"He is working late.\"\n",
        "A: None of the sentences are questions. The answer is False.\n",
        "\n",
        "The sentence in this group is a question: \"What is your favorite color?\", \"I am learning French.\", \"Can you help me?\", \"The car is red.\"\n",
        "A: The sentences \"What is your favorite color?\" and \"Can you help me?\" are questions. The answer is True.\n",
        "\n",
        "The sentence in this group is a question: \"Are you coming to the party?\", \"She is baking a cake.\", \"Where did you put the keys?\", \"They are watching a movie.\"\n",
        "A: The sentences \"Are you coming to the party?\" and \"Where did you put the keys?\" are questions. The answer is True.\n",
        "\n",
        "The sentence in this group is a question: \"He is reading a book.\", \"What time is the meeting?\", \"She enjoys painting.\", \"The dog is barking.\"\n",
        "A:\n",
        "\n",
        "\"\"\"\n",
        "res = openai.Completion.create(\n",
        "    engine='gpt-3.5-turbo-instruct',\n",
        "    prompt=prompt,\n",
        "    max_tokens=10,\n",
        "    temperature=1.0\n",
        ")\n",
        "\n",
        "print(res['choices'][0]['text'].strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o25DKZMzWpHv",
        "outputId": "83d5542d-54e2-4ebf-bbf3-103881d31e74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sentence \"What time is the meeting?\" is a question. The answer is True.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# zero shot COT"
      ],
      "metadata": {
        "id": "2Wg6Ak4nI2Vp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "prompt =\"\"\"\n",
        "Solve for x in the equation: x^1/2 + 39*2 =87.\n",
        "\n",
        "Let's solve this step-by-step:\n",
        "\"\"\"\n",
        "res = openai.Completion.create(\n",
        "    engine='gpt-3.5-turbo-instruct',\n",
        "    prompt=prompt,\n",
        "    max_tokens=256,\n",
        "    temperature=1.0\n",
        ")\n",
        "\n",
        "print(res['choices'][0]['text'].strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQ0iDf29WpLX",
        "outputId": "0b093e6f-e3e8-4b1b-f901-917a56d8af63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1) Start by subtracting 39*2 from both sides:\n",
            "x^1/2 = 87 - 39*2\n",
            "\n",
            "2) Simplify the right side:\n",
            "x^1/2 = 87 - 78\n",
            "\n",
            "3) Combine like terms:\n",
            "x^1/2 = 9\n",
            "\n",
            "4) To get rid of the square root, we need to square both sides:\n",
            "(x^1/2)^2 = 9^2\n",
            "\n",
            "5) Simplify:\n",
            "x = 81\n",
            "\n",
            "So the solution for x is 81.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HsKGgknba31"
      },
      "source": [
        "\n",
        "\n",
        "## Adding Multiple Contexts\n",
        "\n",
        "In some use-cases like question-answering we can use an external source of information to improve the reliability or *factfulness* of model responses. We refer to this information as _\"source knowledge\"_, which is any knowledge fed into the model via the input prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "QhiAZbxMba31"
      },
      "outputs": [],
      "source": [
        "contexts = [\n",
        "    (\n",
        "        \"Large Language Models (LLMs) are the latest models used in NLP. \" +\n",
        "        \"Their superior performance over smaller models has made them incredibly \" +\n",
        "        \"useful for developers building NLP enabled applications. These models \" +\n",
        "        \"can be accessed via Hugging Face's `transformers` library, via OpenAI \" +\n",
        "        \"using the `openai` library, and via Cohere using the `cohere` library.\"\n",
        "    ),\n",
        "    (\n",
        "        \"To use OpenAI's GPT-3 model for completion (generation) tasks, you \" +\n",
        "        \"first need to get an API key from \" +\n",
        "        \"'https://beta.openai.com/account/api-keys'.\"\n",
        "    ),\n",
        "    (\n",
        "        \"OpenAI's API is accessible via Python using the `openai` library. \" +\n",
        "        \"After installing the library with pip you can use it as follows: \\n\" +\n",
        "        \"```import openai\\nopenai.api_key = 'YOUR_API_KEY'\\nprompt = \\n\" +\n",
        "        \"'<YOUR PROMPT>'\\nres = openai.Completion.create(engine='text-davinci\" +\n",
        "        \"-003', prompt=prompt, max_tokens=100)\\nprint(res)\"\n",
        "    ),\n",
        "    (\n",
        "        \"The OpenAI endpoint is available for completion tasks via the \" +\n",
        "        \"LangChain library. To use it, first install the library with \" +\n",
        "        \"`pip install langchain openai`. Then, import the library and \" +\n",
        "        \"initialize the model as follows: \\n\" +\n",
        "        \"```from langchain.llms import OpenAI\\nopenai = OpenAI(\" +\n",
        "        \"model_name='text-davinci-003', openai_api_key='YOUR_API_KEY')\\n\" +\n",
        "        \"prompt = 'YOUR_PROMPT'\\nprint(openai(prompt))```\"\n",
        "    )\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pJ82t7Lba31"
      },
      "source": [
        "We would feed this external information into our prompt between the initial *instructions* and the *user input*. For OpenAI models it's recommended to separate the contexts from the rest of the prompt using `###` or `\"\"\"`, and each independent context can be separated with a few newlines and `##`, like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnUQvuIiba32",
        "outputId": "92fccb2a-d533-4ddd-dee5-1a2c4ee305a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer the question based on the contexts below. If the\n",
            "question cannot be answered using the information provided answer\n",
            "with \"I don't know\".\n",
            "\n",
            "###\n",
            "\n",
            "Contexts:\n",
            "Large Language Models (LLMs) are the latest models used in NLP. Their superior performance over smaller models has made them incredibly useful for developers building NLP enabled applications. These models can be accessed via Hugging Face's `transformers` library, via OpenAI using the `openai` library, and via Cohere using the `cohere` library.\n",
            "\n",
            "##\n",
            "\n",
            "To use OpenAI's GPT-3 model for completion (generation) tasks, you first need to get an API key from 'https://beta.openai.com/account/api-keys'.\n",
            "\n",
            "##\n",
            "\n",
            "OpenAI's API is accessible via Python using the `openai` library. After installing the library with pip you can use it as follows: \n",
            "```import openai\n",
            "openai.api_key = 'YOUR_API_KEY'\n",
            "prompt = \n",
            "'<YOUR PROMPT>'\n",
            "res = openai.Completion.create(engine='text-davinci-003', prompt=prompt, max_tokens=100)\n",
            "print(res)\n",
            "\n",
            "##\n",
            "\n",
            "The OpenAI endpoint is available for completion tasks via the LangChain library. To use it, first install the library with `pip install langchain openai`. Then, import the library and initialize the model as follows: \n",
            "```from langchain.llms import OpenAI\n",
            "openai = OpenAI(model_name='text-davinci-003', openai_api_key='YOUR_API_KEY')\n",
            "prompt = 'YOUR_PROMPT'\n",
            "print(openai(prompt))```\n",
            "\n",
            "###\n",
            "\n",
            "Question: Give me two examples of how to use OpenAI's GPT-3 model\n",
            "using Python from start to finish\n",
            "\n",
            "Answer: \n"
          ]
        }
      ],
      "source": [
        "context_str = '\\n\\n##\\n\\n'.join(contexts)\n",
        "\n",
        "print(f\"\"\"Answer the question based on the contexts below. If the\n",
        "question cannot be answered using the information provided answer\n",
        "with \"I don't know\".\n",
        "\n",
        "###\n",
        "\n",
        "Contexts:\n",
        "{context_str}\n",
        "\n",
        "###\n",
        "\n",
        "Question: Give me two examples of how to use OpenAI's GPT-3 model\n",
        "using Python from start to finish\n",
        "\n",
        "Answer: \"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UMY42yAba32",
        "outputId": "dec3027f-73c3-4f76-b911-96c8ea355996"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. import openai\n",
            "openai.api_key = 'YOUR_API_KEY'\n",
            "prompt = '<YOUR PROMPT>'\n",
            "res = openai.Completion.create(engine='text-davinci-003', prompt=prompt, max_tokens=100)\n",
            "print(res)\n",
            "\n",
            "2. from langchain.llms import OpenAI\n",
            "openai = OpenAI(model_name='text-davinci-003', openai_api_key='YOUR_API_KEY')\n",
            "prompt = 'YOUR_PROMPT'\n",
            "print(openai(prompt))\n"
          ]
        }
      ],
      "source": [
        "prompt = f\"\"\"Answer the question based on the contexts below. If the\n",
        "question cannot be answered using the information provided answer\n",
        "with \"I don't know\".\n",
        "\n",
        "###\n",
        "\n",
        "Contexts:\n",
        "{context_str}\n",
        "\n",
        "###\n",
        "\n",
        "Question: Give me two examples of how to use OpenAI's GPT-3 model\n",
        "using Python from start to finish\n",
        "\n",
        "Answer: \"\"\"\n",
        "\n",
        "res = openai.Completion.create(\n",
        "    engine='gpt-3.5-turbo-instruct',\n",
        "    prompt=prompt,\n",
        "    max_tokens=256,\n",
        "    temperature=0.0\n",
        ")\n",
        "\n",
        "print(res['choices'][0]['text'].strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScZ9qE-Lba32"
      },
      "source": [
        "Not bad, but are these contexts actually helping? Maybe the model is able to answer these questions without the additional information (source knowledge) as is able to rely solely on information stored within the model's internal parameters (parametric knowledge). Let's ask again without the external information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alevc66Kba32",
        "outputId": "73d67832-f531-407d-afd0-13e99070a334"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. Using OpenAI's GPT-3 model with Python to generate text: \n",
            "    - Install the OpenAI Python package\n",
            "    - Load the GPT-3 model\n",
            "    - Generate text using the GPT-3 model\n",
            "\n",
            "2. Using OpenAI's GPT-3 model with Python to generate images: \n",
            "    - Install the OpenAI Python package\n",
            "    - Load the GPT-3 model\n",
            "    - Generate images using the GPT-3 model\n"
          ]
        }
      ],
      "source": [
        "prompt = f\"\"\"Answer the question based on the contexts below. If the\n",
        "question cannot be answered using the information provided answer\n",
        "with \"I don't know\".\n",
        "\n",
        "Question: Give me two examples of how to use OpenAI's GPT-3 model\n",
        "using Python from start to finish\n",
        "\n",
        "Answer: \"\"\"\n",
        "\n",
        "res = openai.Completion.create(\n",
        "    engine='gpt-3.5-turbo-instruct',\n",
        "    prompt=prompt,\n",
        "    max_tokens=256,\n",
        "    temperature=0.0\n",
        ")\n",
        "\n",
        "print(res['choices'][0]['text'].strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_UCtq-qba32"
      },
      "source": [
        "These are not really what we asked for, and are definitely not very specific. So clearly adding some source knowledge to our prompts can result in some much better results.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LjJlNLZi3gVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Maximum Prompt Sizes\n"
      ],
      "metadata": {
        "id": "9nOkdjvb3ibp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Z3F1ht1Dba32"
      },
      "outputs": [],
      "source": [
        "!pip install -qU tiktoken==0.4.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNgL_NPLba33",
        "outputId": "4451b054-7598-4f2b-a803-26e083b7ad79"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "412"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "import tiktoken\n",
        "\n",
        "prompt = f\"\"\"Answer the question based on the contexts below. If the\n",
        "question cannot be answered using the information provided answer\n",
        "with \"I don't know\".\n",
        "\n",
        "###\n",
        "\n",
        "Contexts:\n",
        "{'##'.join(contexts)}\n",
        "\n",
        "###\n",
        "\n",
        "Question: Give me two examples of how to use OpenAI's GPT-3 model\n",
        "using Python from start to finish\n",
        "\n",
        "Answer: \"\"\"\n",
        "\n",
        "encoder_name = 'p50k_base'\n",
        "tokenizer = tiktoken.get_encoding(encoder_name)\n",
        "\n",
        "len(tokenizer.encode(prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v396Uo2Aba33"
      },
      "source": [
        "When feeding this prompt into `gpt-3.5-turbo-instruct` it will use `412` of  maximum context window of `4097`, leaving us with `4097 - 412 == 3685` tokens for  completion.\n",
        "\n",
        "\n",
        "\n",
        "By default the maximum number of tokens used for completion is `256`. We can increase this upto the maximum calculated above of `3685`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b0MPOw_ba33",
        "outputId": "2166d76c-acdc-41a9-d7e0-299b8c6097ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. First, you need to get an API key from 'https://beta.openai.com/account/api-keys'. Then, you can use the `openai` library to access the GPT-3 model. After installing the library with pip, you can use it as follows:\n",
            "```import openai\n",
            "openai.api_key = 'YOUR_API_KEY'\n",
            "prompt = '<YOUR PROMPT>'\n",
            "res = openai.Completion.create(engine='text-davinci-003', prompt=prompt, max_tokens=100)\n",
            "print(res)```\n",
            "\n",
            "2. Another way to use OpenAI's GPT-3 model is through the LangChain library. First, install the library with `pip install langchain openai`. Then, import the library and initialize the model as follows:\n",
            "```from langchain.llms import OpenAI\n",
            "openai = OpenAI(model_name='text-davinci-003', openai_api_key='YOUR_API_KEY')\n",
            "prompt = 'YOUR_PROMPT'\n",
            "print(openai(prompt))```\n"
          ]
        }
      ],
      "source": [
        "res = openai.Completion.create(\n",
        "    engine='gpt-3.5-turbo-instruct',\n",
        "    prompt=prompt,\n",
        "    temperature=0.0,\n",
        "    max_tokens=3685\n",
        ")\n",
        "\n",
        "print(res['choices'][0]['text'].strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSVA6J_Yba33"
      },
      "source": [
        "The model doesn't need the full size of completion and doesn't try to fill the full space, but because we increased the value of `openai.max_tokens`, inference does take notably longer.\n",
        "\n",
        "If we exceed the maximum context window allowed, we'll see an error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqVb2qlqba33",
        "outputId": "9270781d-8e7e-4da7-c57d-2d5e9c9d6944"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This model's maximum context length is 4097 tokens, however you requested 9150 tokens (351 in your prompt; 8799 for the completion). Please reduce your prompt; or completion length.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    res = openai.Completion.create(\n",
        "        engine='gpt-3.5-turbo-instruct',\n",
        "        prompt=prompt,\n",
        "        temperature=0.0,\n",
        "        max_tokens=8799\n",
        "    )\n",
        "except openai.InvalidRequestError as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfY1mT8wba33"
      },
      "source": [
        "So it can be a good idea to integrate this type of check into our code if we expect to exceed the maximum context window at any point."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "b8e7999f96e1b425e2d542f21b571f5a4be3e97158b0b46ea1b2500df63956ce"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}