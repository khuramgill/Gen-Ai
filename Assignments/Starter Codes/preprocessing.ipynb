{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69016190-5841-4d7c-95cf-83c91ccb07a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54c3597-1c44-412b-9d81-73253afe1c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/HP/Desktop/Bootcamp_codes\\Week1\\IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f732e804-c905-4246-8261-1f9343fb8274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f51852-679c-4e27-ac25-d5783f5b53db",
   "metadata": {},
   "source": [
    "# Case Folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3e6245c-9400-4d4b-a4ac-abc0a0b44d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'probably my all-time favorite movie, a story of selflessness, sacrifice and dedication to a noble cause, but it\\'s not preachy or boring. it just never gets old, despite my having seen it some 15 or more times in the last 25 years. paul lukas\\' performance brings tears to my eyes, and bette davis, in one of her very few truly sympathetic roles, is a delight. the kids are, as grandma says, more like \"dressed-up midgets\" than children, but that only makes them more fun to watch. and the mother\\'s slow awakening to what\\'s happening in the world and under her own roof is believable and startling. if i had a dozen thumbs, they\\'d all be \"up\" for this movie.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'][5].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d04bd68d-008d-4557-be20-180428edfb62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91ca5ceb-526a-4f07-be0c-bf1d4ec5381e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review']=df['review'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4337fcd1-5ef3-49d1-b998-e5790a3dc7d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production. &lt;br /&gt;&lt;br /&gt;the...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei's \"love in the time of money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>i sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>this show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>if you like original gut wrenching laughter yo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  one of the other reviewers has mentioned that ...  positive\n",
       "1  a wonderful little production. <br /><br />the...  positive\n",
       "2  i thought this was a wonderful way to spend ti...  positive\n",
       "3  basically there's a family where a little boy ...  negative\n",
       "4  petter mattei's \"love in the time of money\" is...  positive\n",
       "5  probably my all-time favorite movie, a story o...  positive\n",
       "6  i sure would like to see a resurrection of a u...  positive\n",
       "7  this show was an amazing, fresh & innovative i...  negative\n",
       "8  encouraged by the positive comments about this...  negative\n",
       "9  if you like original gut wrenching laughter yo...  positive"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8747730c-d766-479b-9f22-11bdbc2afa52",
   "metadata": {},
   "source": [
    "# Remove tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8dd4e1b-cdf2-48bc-b14f-7598504de721",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_tags(text):\n",
    "    pattern = re.compile('<.*?>')\n",
    "    return pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f994ad6-9ffc-4780-a56c-156e8cf89b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. <br /><br />The actors are extremely well chosen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "505dd797-1eca-4e14-a976-3e01dc226891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A wonderful little production. The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. The actors are extremely well chosen\n"
     ]
    }
   ],
   "source": [
    "print(remove_tags(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2088b26-3a19-4c86-8f42-2d4b8d438b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review']=df['review'].apply(remove_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a5abc82-79b5-4700-9ea3-09f6e7612751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"basically there's a family where a little boy (jake) thinks there's a zombie in his closet & his parents are fighting all the time.this movie is slower than a soap opera... and suddenly, jake decides to become rambo and kill the zombie.ok, first of all when you're going to make a film you must decide if its a thriller or a drama! as a drama the movie is watchable. parents are divorcing & arguing like in real life. and then we have jake with his closet which totally ruins all the film! i expected to see a boogeyman similar movie, and instead i watched a drama with some meaningless thriller spots.3 out of 10 just for the well playing parents & descent dialogs. as for the shots with jake: just ignore them.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada70b32-acde-4f86-a03f-62d399ee0695",
   "metadata": {},
   "source": [
    "# Remove URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "460be3d7-36d1-4c23-82a3-a31da3e8ed81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(url):\n",
    "    pattern = re.compile(r'https?://\\S+|www\\.\\S')\n",
    "    return pattern.sub(r'', url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76bfbdfa-a646-4f06-9e1e-31683937fa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "url1 = 'this is my scholar profile, check it out: https://scholar.google.com/citations?user=WM9A5m4AAAAJ&hl=en&oi=sra'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62b0ab32-a535-4312-8783-4038116d4126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is my scholar profile, check it out: \n"
     ]
    }
   ],
   "source": [
    "print(remove_url(url1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dc36b0-351e-490a-8c64-2ead908004a8",
   "metadata": {},
   "source": [
    "# Remove Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5404cad-0771-4222-839b-1e7fedb67ba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string, time\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58900afe-8c48-4125-ada6-4c512305caa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b7f93ed-7f05-4b3b-85a8-d6179bea0789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    for char in exclude:\n",
    "        text = text.replace(char, '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aca07c16-d9f4-46f1-ba5c-42811719c636",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = 'is. this/ the? string/ with} any punctuation?!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09e9194f-ee22-4995-afc6-a63c01e5bd6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is this the string with any punctuation\n"
     ]
    }
   ],
   "source": [
    "print(remove_punctuation(text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56232570-4845-4678-bb57-c94d86eeef63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation2(text):\n",
    "    return text.translate(str.maketrans('','', exclude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "82adc910-226c-450c-82ac-ce6ab37b04c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is this the string with any punctuation'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_punctuation2(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "220c1fbb-40b3-4b63-957c-d9294a50559f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=pd.read_csv('C:/Users/HP/Desktop/Bootcamp_codes/Week1/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dcfd94d9-4298-44df-b661-a4b3b4ad301c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10360</th>\n",
       "      <td>10361</td>\n",
       "      <td>0</td>\n",
       "      <td>@user there are people that \"don't like me\" a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19283</th>\n",
       "      <td>19284</td>\n",
       "      <td>0</td>\n",
       "      <td>staed my day out with a brief moment of yoga &amp;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19145</th>\n",
       "      <td>19146</td>\n",
       "      <td>1</td>\n",
       "      <td>@user @user is this what your university stand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6332</th>\n",
       "      <td>6333</td>\n",
       "      <td>1</td>\n",
       "      <td>@user please don't forget to use the word ! th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16803</th>\n",
       "      <td>16804</td>\n",
       "      <td>1</td>\n",
       "      <td>@user #allahsoil we think of cultures as separ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  label                                              tweet\n",
       "10360  10361      0   @user there are people that \"don't like me\" a...\n",
       "19283  19284      0  staed my day out with a brief moment of yoga &...\n",
       "19145  19146      1  @user @user is this what your university stand...\n",
       "6332    6333      1  @user please don't forget to use the word ! th...\n",
       "16803  16804      1  @user #allahsoil we think of cultures as separ..."
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ad4c8c0-1101-4151-95bf-f8c3c910616c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['tweet'] = df2['tweet'].apply(remove_punctuation2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c7b384c6-b26d-4bb1-9f71-6c76c3e3a9d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9280</th>\n",
       "      <td>9281</td>\n",
       "      <td>0</td>\n",
       "      <td>black12iwhiteboy  na porn nude xxx nasty young...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15089</th>\n",
       "      <td>15090</td>\n",
       "      <td>1</td>\n",
       "      <td>user prison officers goaded muslim inmates unt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31476</th>\n",
       "      <td>31477</td>\n",
       "      <td>0</td>\n",
       "      <td>to announce  the first endangered butterflie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10151</th>\n",
       "      <td>10152</td>\n",
       "      <td>0</td>\n",
       "      <td>90 days to my first race rio2016 gocanada   mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18412</th>\n",
       "      <td>18413</td>\n",
       "      <td>1</td>\n",
       "      <td>its time to end  now days of our lives live sh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  label                                              tweet\n",
       "9280    9281      0  black12iwhiteboy  na porn nude xxx nasty young...\n",
       "15089  15090      1  user prison officers goaded muslim inmates unt...\n",
       "31476  31477      0    to announce  the first endangered butterflie...\n",
       "10151  10152      0  90 days to my first race rio2016 gocanada   mo...\n",
       "18412  18413      1  its time to end  now days of our lives live sh..."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c8b404-427d-4e89-a926-90a116efab43",
   "metadata": {},
   "source": [
    "# Replace Short Hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a892f3d7-94c2-4881-98c3-06fa3b99b326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AFAIK': 'As Far As I Know', 'AFK': 'Away From Keyboard', 'ASAP': 'As Soon As Possible', 'ATK': 'At The Keyboard', 'ATM': 'At The Moment', 'A3': 'Anytime, Anywhere, Anyplace', 'BAK': 'Back At Keyboard', 'BBL': 'Be Back Later', 'BBS': 'Be Back Soon', 'BFN': 'Bye For Now', 'B4N': 'Bye For Now', 'BRB': 'Be Right Back', 'BRT': 'Be Right There', 'BTW': 'By The Way', 'B4': 'Before', 'CU': 'See You', 'CUL8R': 'See You Later', 'CYA': 'See You', 'FAQ': 'Frequently Asked Questions', 'FC': 'Fingers Crossed', 'FWIW': \"For What It's Worth\", 'FYI': 'For Your Information', 'GAL': 'Get A Life', 'GG': 'Good Game', 'GN': 'Good Night', 'GMTA': 'Great Minds Think Alike', 'GR8': 'Great!', 'G9': 'Genius', 'IC': 'I See', 'ICQ': 'I Seek you (also a chat program)', 'ILU': 'ILU: I Love You', 'IMHO': 'In My Honest/Humble Opinion', 'IMO': 'In My Opinion', 'IOW': 'In Other Words', 'IRL': 'In Real Life', 'KISS': 'Keep It Simple, Stupid', 'LDR': 'Long Distance Relationship', 'LMAO': 'Laugh My A.. Off', 'LOL': 'Laughing Out Loud', 'LTNS': 'Long Time No See', 'L8R': 'Later', 'MTE': 'My Thoughts Exactly', 'M8': 'Mate', 'NRN': 'No Reply Necessary', 'OIC': 'Oh I See', 'PITA': 'Pain In The A..', 'PRT': 'Party', 'PRW': 'Parents Are Watching', 'ROFL': 'Rolling On The Floor Laughing', 'ROFLOL': 'Rolling On The Floor Laughing Out Loud', 'ROTFLMAO': 'Rolling On The Floor Laughing My A.. Off', 'SK8': 'Skate', 'STATS': 'Your sex and age', 'ASL': 'Age, Sex, Location', 'THX': 'Thank You', 'TTFN': 'Ta-Ta For Now!', 'TTYL': 'Talk To You Later', 'U': 'You', 'U2': 'You Too', 'U4E': 'Yours For Ever', 'WB': 'Welcome Back', 'WTF': 'What The F...', 'WTG': 'Way To Go!', 'WUF': 'Where Are You From?', 'W8': 'Wait...', '7K': 'Sick:-D Laugher'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "with open('D:\\Learning\\Gen-Ai\\Data Set\\slang.txt', 'r') as file:\n",
    "    data_dict = {}\n",
    "\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        if line and '=' in line:\n",
    "            key, value = line.split('=', 1)\n",
    "            data_dict[key] = value\n",
    "\n",
    "print(data_dict)\n",
    "# print(\"Hello Slang\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7b4a12a4-b14f-4529-93cb-22c6aabc8517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_conversion(text):\n",
    "    new_text =[]\n",
    "    for c in text.split():\n",
    "        if c.upper() in data_dict:\n",
    "            new_text.append(data_dict[c.upper()])\n",
    "        else:\n",
    "            new_text.append(c)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "05847a62-5b03-4046-a2fb-2c7a25856026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In My Honest/Humble Opinion he is the best'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_conversion('IMHO he is the best')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59ece92-6202-4412-9ee6-3c12c38d6ac8",
   "metadata": {},
   "source": [
    "# Correct Incorrect words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfbffce3-2a29-47d7-8d12-2849380378a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: TextBlob in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.18.0.post0)\n",
      "Requirement already satisfied: nltk>=3.8 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from TextBlob) (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk>=3.8->TextBlob) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk>=3.8->TextBlob) (1.4.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk>=3.8->TextBlob) (2024.4.16)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk>=3.8->TextBlob) (4.66.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click->nltk>=3.8->TextBlob) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "# !pip install TextBlob\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "588ea888-3ab3-462e-a51f-aca9958bf3b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A large language model is a computational model notable for its ability to active general-purpose language generation and other natural language processing tasks such as classification.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorr_text = \"A large language model is a computational model notablee for its aability to achiv general-purpose language generation and other natural language processing tasks such as classification.\"\n",
    "textBlb = TextBlob(incorr_text)\n",
    "textBlb.correct().string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3282fe1f-baa9-4e81-9bfa-e49627c75270",
   "metadata": {},
   "source": [
    "# Stop words Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f0e33179-e981-4c14-ba1c-a82444426867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install nltk==3.5\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "43606994-5c30-47a8-a703-fca71ce8b410",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eac892c2-bc61-4faf-ab35-a43917a7c2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')     #to download the necessary stopwords data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2c5f2282-fc68-4b59-9adb-ff3e87eb9890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e6c59d4a-d1b3-4254-91e7-3587e36df02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    new_text = []\n",
    "    for word in text.split():\n",
    "        if word in stopwords.words('english'):\n",
    "            new_text.append('')\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    x = new_text[:]\n",
    "    new_text.clear()\n",
    "    return \" \".join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1238546f-b998-4032-b6c6-fb3a7cbbf4be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' thought    wonderful way  spend time    hot summer weekend, sitting   air conditioned theater  watching  light-hearted comedy.  plot  simplistic,   dialogue  witty   characters  likable (even  well bread suspected serial killer).   may  disappointed   realize    match point 2: risk addiction,  thought   proof  woody allen  still fully  control   style many  us  grown  love.this    i\\'d laughed  one  woody\\'s comedies  years (dare  say  decade?).  i\\'ve never  impressed  scarlet johanson,    managed  tone   \"sexy\" image  jumped right   average,  spirited young woman.this may    crown jewel   career,    wittier  \"devil wears prada\"   interesting  \"superman\"  great comedy  go see  friends.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(df['review'][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc89a64-77f7-4a9d-92a5-64731564de19",
   "metadata": {},
   "source": [
    "# Emoji Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b4c0383e-a407-4bd0-9ca0-e91c3bb35a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # Emotions\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # Symbols/Pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # Transport and map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # Flags\n",
    "        u\"\\u2702-\\u27B0\"          # Other symbols\n",
    "        u\"\\u24C2-\\U0001F251\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b85956aa-4741-45e3-a012-132c80636f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python is \n"
     ]
    }
   ],
   "source": [
    "result = remove_emoji(\"Python is 🔥\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20b74cf0-8bb0-435e-8665-95e762b5381b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emoji\n",
      "  Downloading emoji-2.12.1-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.7.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from emoji) (4.11.0)\n",
      "Downloading emoji-2.12.1-py3-none-any.whl (431 kB)\n",
      "   ---------------------------------------- 0.0/431.4 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 30.7/431.4 kB 1.4 MB/s eta 0:00:01\n",
      "   -- ------------------------------------- 30.7/431.4 kB 1.4 MB/s eta 0:00:01\n",
      "   -- ------------------------------------- 30.7/431.4 kB 1.4 MB/s eta 0:00:01\n",
      "   -- ------------------------------------- 30.7/431.4 kB 1.4 MB/s eta 0:00:01\n",
      "   --------- ---------------------------- 112.6/431.4 kB 469.7 kB/s eta 0:00:01\n",
      "   --------- ---------------------------- 112.6/431.4 kB 469.7 kB/s eta 0:00:01\n",
      "   --------- ---------------------------- 112.6/431.4 kB 469.7 kB/s eta 0:00:01\n",
      "   --------- ---------------------------- 112.6/431.4 kB 469.7 kB/s eta 0:00:01\n",
      "   --------------------- ---------------- 245.8/431.4 kB 558.1 kB/s eta 0:00:01\n",
      "   --------------------- ---------------- 245.8/431.4 kB 558.1 kB/s eta 0:00:01\n",
      "   --------------------- ---------------- 245.8/431.4 kB 558.1 kB/s eta 0:00:01\n",
      "   --------------------- ---------------- 245.8/431.4 kB 558.1 kB/s eta 0:00:01\n",
      "   ------------------------ ------------- 276.5/431.4 kB 426.0 kB/s eta 0:00:01\n",
      "   -------------------------------- ----- 368.6/431.4 kB 560.0 kB/s eta 0:00:01\n",
      "   -------------------------------- ----- 368.6/431.4 kB 560.0 kB/s eta 0:00:01\n",
      "   -------------------------------- ----- 368.6/431.4 kB 560.0 kB/s eta 0:00:01\n",
      "   -------------------------------- ----- 368.6/431.4 kB 560.0 kB/s eta 0:00:01\n",
      "   -------------------------------- ----- 368.6/431.4 kB 560.0 kB/s eta 0:00:01\n",
      "   -------------------------------------  430.1/431.4 kB 471.7 kB/s eta 0:00:01\n",
      "   -------------------------------------- 431.4/431.4 kB 456.7 kB/s eta 0:00:00\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-2.12.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6056b471-8c93-4221-9a32-450b8e507451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python is :fire:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import emoji \n",
    "print(emoji.demojize('python is 🔥'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fd5a58-d279-4cd0-bccd-5a7216ff8a46",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "625640d8-d274-4bab-a5a7-46f076bfdcca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'UK']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence1= 'I am going to UK'\n",
    "sentence1.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3af4fdba-3585-4b56-9807-f8d9b9669024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am going to UK', ' I will stay there for 10 days', ' Good Bye!']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence2=\"I am going to UK. I will stay there for 10 days. Good Bye!\"\n",
    "sentence2.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7e5ae015-f45c-4c0b-be16-2eea282ed812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what is your name? i think i have seen you somewhere']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence3=\"what is your name? i think i have seen you somewhere\"\n",
    "sentence3.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73023931-d923-466b-8df9-e5c4e0e13f46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'UK']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use regex\n",
    "import re\n",
    "snt4= 'I am going to UK'\n",
    "tokens = re.findall(\"[\\w']+\", snt4)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ce129321-92c2-4943-8255-8029317e761d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Download the 'punkt' tokenizer model\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8cc63574-639d-4e3f-9914-41fb7224f21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using NLTK\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "efd86f2e-20c2-4653-b39a-bea4cba46d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_string = \"Muad Dib learned rapidly because his first training was in how to learn. And the first lesson of all was the basic trust that he could learn. It is shocking to find how many people do not believe they can learn, and how many more believe learning to be difficult.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c3ed86b1-d94c-4408-a217-2fa0f7b474ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Muad Dib learned rapidly because his first training was in how to learn.',\n",
       " 'And the first lesson of all was the basic trust that he could learn.',\n",
       " 'It is shocking to find how many people do not believe they can learn, and how many more believe learning to be difficult.']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(example_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "72c8856f-f822-41c1-a105-a5b6172b547b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Muad',\n",
       " 'Dib',\n",
       " 'learned',\n",
       " 'rapidly',\n",
       " 'because',\n",
       " 'his',\n",
       " 'first',\n",
       " 'training',\n",
       " 'was',\n",
       " 'in',\n",
       " 'how',\n",
       " 'to',\n",
       " 'learn',\n",
       " '.',\n",
       " 'And',\n",
       " 'the',\n",
       " 'first',\n",
       " 'lesson',\n",
       " 'of',\n",
       " 'all',\n",
       " 'was',\n",
       " 'the',\n",
       " 'basic',\n",
       " 'trust',\n",
       " 'that',\n",
       " 'he',\n",
       " 'could',\n",
       " 'learn',\n",
       " '.',\n",
       " 'It',\n",
       " 'is',\n",
       " 'shocking',\n",
       " 'to',\n",
       " 'find',\n",
       " 'how',\n",
       " 'many',\n",
       " 'people',\n",
       " 'do',\n",
       " 'not',\n",
       " 'believe',\n",
       " 'they',\n",
       " 'can',\n",
       " 'learn',\n",
       " ',',\n",
       " 'and',\n",
       " 'how',\n",
       " 'many',\n",
       " 'more',\n",
       " 'believe',\n",
       " 'learning',\n",
       " 'to',\n",
       " 'be',\n",
       " 'difficult',\n",
       " '.']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(example_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "64ad7d97-f37e-4748-9bae-90ef1a7dd661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.7.4-cp310-cp310-win_amd64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.10-cp310-cp310-win_amd64.whl.metadata (2.0 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.8-cp310-cp310-win_amd64.whl.metadata (8.6 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.9-cp310-cp310-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.3.0,>=8.2.2 (from spacy)\n",
      "  Downloading thinc-8.2.3-cp310-cp310-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.2-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.4.8-cp310-cp310-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.4.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.3.4-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting typer<0.10.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.9.4-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open<7.0.0,>=5.2.1 (from spacy)\n",
      "  Downloading smart_open-6.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Downloading pydantic-2.7.1-py3-none-any.whl.metadata (107 kB)\n",
      "     ---------------------------------------- 0.0/107.3 kB ? eta -:--:--\n",
      "     -------------- ---------------------- 41.0/107.3 kB 667.8 kB/s eta 0:00:01\n",
      "     -------------- ---------------------- 41.0/107.3 kB 667.8 kB/s eta 0:00:01\n",
      "     ---------------------------- -------- 81.9/107.3 kB 512.0 kB/s eta 0:00:01\n",
      "     ------------------------------------ 107.3/107.3 kB 622.3 kB/s eta 0:00:00\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (3.1.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (63.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (24.0)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.4.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.2.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.18.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading pydantic_core-2.18.2-cp310-none-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Downloading blis-0.7.11-cp310-cp310-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Downloading confection-0.1.4-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
      "Collecting cloudpathlib<0.17.0,>=0.7.0 (from weasel<0.4.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.16.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->spacy) (2.1.5)\n",
      "Collecting marisa-trie>=0.7.7 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.1.1-cp310-cp310-win_amd64.whl.metadata (8.9 kB)\n",
      "Downloading spacy-3.7.4-cp310-cp310-win_amd64.whl (12.1 MB)\n",
      "   ---------------------------------------- 0.0/12.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/12.1 MB 3.2 MB/s eta 0:00:04\n",
      "   ---------------------------------------- 0.1/12.1 MB 3.2 MB/s eta 0:00:04\n",
      "    --------------------------------------- 0.2/12.1 MB 1.2 MB/s eta 0:00:10\n",
      "    --------------------------------------- 0.2/12.1 MB 1.5 MB/s eta 0:00:08\n",
      "    --------------------------------------- 0.2/12.1 MB 1.5 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 0.3/12.1 MB 1.2 MB/s eta 0:00:10\n",
      "   - -------------------------------------- 0.5/12.1 MB 1.7 MB/s eta 0:00:07\n",
      "   - -------------------------------------- 0.5/12.1 MB 1.7 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 0.8/12.1 MB 1.9 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 1.1/12.1 MB 2.4 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 1.2/12.1 MB 2.4 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 1.8/12.1 MB 3.2 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 2.3/12.1 MB 3.8 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 2.6/12.1 MB 4.0 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 3.1/12.1 MB 4.6 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.7/12.1 MB 5.0 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 4.3/12.1 MB 5.4 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 4.3/12.1 MB 5.4 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 4.3/12.1 MB 5.4 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 4.3/12.1 MB 5.4 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 4.3/12.1 MB 5.4 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 4.5/12.1 MB 4.4 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 5.4/12.1 MB 5.1 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 5.5/12.1 MB 5.2 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 5.8/12.1 MB 5.0 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 6.3/12.1 MB 5.2 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 6.5/12.1 MB 5.3 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 6.9/12.1 MB 5.3 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 7.4/12.1 MB 5.5 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 7.9/12.1 MB 5.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 8.8/12.1 MB 6.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 9.3/12.1 MB 6.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 9.8/12.1 MB 6.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.3/12.1 MB 6.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.9/12.1 MB 8.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.4/12.1 MB 8.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.0/12.1 MB 8.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.1/12.1 MB 8.6 MB/s eta 0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.8-cp310-cp310-win_amd64.whl (39 kB)\n",
      "Downloading langcodes-3.4.0-py3-none-any.whl (182 kB)\n",
      "   ---------------------------------------- 0.0/182.0 kB ? eta -:--:--\n",
      "   --------------------------------------- 182.0/182.0 kB 10.7 MB/s eta 0:00:00\n",
      "Downloading murmurhash-1.0.10-cp310-cp310-win_amd64.whl (25 kB)\n",
      "Downloading preshed-3.0.9-cp310-cp310-win_amd64.whl (122 kB)\n",
      "   ---------------------------------------- 0.0/122.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 122.2/122.2 kB 7.0 MB/s eta 0:00:00\n",
      "Downloading pydantic-2.7.1-py3-none-any.whl (409 kB)\n",
      "   ---------------------------------------- 0.0/409.3 kB ? eta -:--:--\n",
      "   --------------------------------------- 409.3/409.3 kB 12.9 MB/s eta 0:00:00\n",
      "Downloading pydantic_core-2.18.2-cp310-none-win_amd64.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.5/1.9 MB 10.5 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.1/1.9 MB 11.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.6/1.9 MB 11.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.9/1.9 MB 11.1 MB/s eta 0:00:00\n",
      "Downloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
      "   ---------------------------------------- 0.0/57.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 57.0/57.0 kB ? eta 0:00:00\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.4.8-cp310-cp310-win_amd64.whl (481 kB)\n",
      "   ---------------------------------------- 0.0/481.9 kB ? eta -:--:--\n",
      "   --------------------------------------  481.3/481.9 kB 10.0 MB/s eta 0:00:01\n",
      "   --------------------------------------- 481.9/481.9 kB 10.0 MB/s eta 0:00:00\n",
      "Downloading thinc-8.2.3-cp310-cp310-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------- ----------------------- 0.6/1.5 MB 12.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.1/1.5 MB 12.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 11.8 MB/s eta 0:00:00\n",
      "Downloading typer-0.9.4-py3-none-any.whl (45 kB)\n",
      "   ---------------------------------------- 0.0/46.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 46.0/46.0 kB ? eta 0:00:00\n",
      "Downloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.3.4-py3-none-any.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.1/50.1 kB 2.7 MB/s eta 0:00:00\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading blis-0.7.11-cp310-cp310-win_amd64.whl (6.6 MB)\n",
      "   ---------------------------------------- 0.0/6.6 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.5/6.6 MB 10.9 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 1.1/6.6 MB 11.2 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 1.6/6.6 MB 11.3 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 1.8/6.6 MB 11.4 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 1.8/6.6 MB 11.4 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 1.8/6.6 MB 11.4 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 1.8/6.6 MB 11.4 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 1.8/6.6 MB 11.4 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 1.8/6.6 MB 11.4 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 1.8/6.6 MB 11.4 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 1.8/6.6 MB 11.4 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 1.8/6.6 MB 11.4 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 1.8/6.6 MB 11.4 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 1.8/6.6 MB 11.4 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 1.8/6.6 MB 11.4 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 1.8/6.6 MB 11.4 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 1.8/6.6 MB 2.4 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 1.8/6.6 MB 2.4 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 1.8/6.6 MB 2.1 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 1.8/6.6 MB 2.1 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 1.8/6.6 MB 2.1 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 1.8/6.6 MB 1.8 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 1.8/6.6 MB 1.8 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 1.8/6.6 MB 1.8 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 1.8/6.6 MB 1.6 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 1.8/6.6 MB 1.6 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 1.8/6.6 MB 1.6 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 1.9/6.6 MB 1.5 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 1.9/6.6 MB 1.5 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 1.9/6.6 MB 1.5 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 2.0/6.6 MB 1.4 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 2.0/6.6 MB 1.4 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 2.1/6.6 MB 1.4 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 2.1/6.6 MB 1.4 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 2.1/6.6 MB 1.4 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 2.3/6.6 MB 1.4 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 2.3/6.6 MB 1.4 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 2.3/6.6 MB 1.4 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 2.5/6.6 MB 1.4 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 2.5/6.6 MB 1.4 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 2.6/6.6 MB 1.3 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 2.8/6.6 MB 1.4 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 2.8/6.6 MB 1.4 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 3.0/6.6 MB 1.4 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 3.2/6.6 MB 1.5 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 3.2/6.6 MB 1.5 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 3.6/6.6 MB 1.6 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 3.8/6.6 MB 1.7 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 3.8/6.6 MB 1.6 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 4.3/6.6 MB 1.8 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 4.4/6.6 MB 1.9 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 4.6/6.6 MB 1.9 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 4.9/6.6 MB 1.9 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 4.9/6.6 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 5.0/6.6 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 5.3/6.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 5.8/6.6 MB 2.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.4/6.6 MB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.6/6.6 MB 2.4 MB/s eta 0:00:00\n",
      "Downloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
      "   ---------------------------------------- 0.0/45.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 45.0/45.0 kB 2.2 MB/s eta 0:00:00\n",
      "Downloading confection-0.1.4-py3-none-any.whl (35 kB)\n",
      "Downloading language_data-1.2.0-py3-none-any.whl (5.4 MB)\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.5/5.4 MB 14.2 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 1.0/5.4 MB 12.9 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 1.5/5.4 MB 12.3 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 2.1/5.4 MB 12.2 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 2.7/5.4 MB 12.1 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 3.2/5.4 MB 12.7 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 3.7/5.4 MB 12.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 4.3/5.4 MB 12.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 4.8/5.4 MB 12.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  5.4/5.4 MB 12.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.4/5.4 MB 11.9 MB/s eta 0:00:00\n",
      "Downloading marisa_trie-1.1.1-cp310-cp310-win_amd64.whl (152 kB)\n",
      "   ---------------------------------------- 0.0/152.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 152.7/152.7 kB 9.5 MB/s eta 0:00:00\n",
      "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, smart-open, pydantic-core, murmurhash, marisa-trie, cloudpathlib, catalogue, blis, annotated-types, typer, srsly, pydantic, preshed, language-data, langcodes, confection, weasel, thinc, spacy\n",
      "Successfully installed annotated-types-0.7.0 blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.16.0 confection-0.1.4 cymem-2.0.8 langcodes-3.4.0 language-data-1.2.0 marisa-trie-1.1.1 murmurhash-1.0.10 preshed-3.0.9 pydantic-2.7.1 pydantic-core-2.18.2 smart-open-6.4.0 spacy-3.7.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.3 typer-0.9.4 wasabi-1.1.2 weasel-0.3.4\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4f528c4d-fadf-4d28-bf0d-e9fe0fa2510f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5538e1e6-4f78-4cf1-b092-49d719f07e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.1/12.8 MB 2.2 MB/s eta 0:00:06\n",
      "      --------------------------------------- 0.3/12.8 MB 2.6 MB/s eta 0:00:05\n",
      "     -- ------------------------------------- 0.6/12.8 MB 4.5 MB/s eta 0:00:03\n",
      "     --- ------------------------------------ 1.2/12.8 MB 6.4 MB/s eta 0:00:02\n",
      "     ----- ---------------------------------- 1.8/12.8 MB 7.5 MB/s eta 0:00:02\n",
      "     ------- -------------------------------- 2.3/12.8 MB 8.2 MB/s eta 0:00:02\n",
      "     -------- ------------------------------- 2.9/12.8 MB 8.7 MB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 3.4/12.8 MB 9.1 MB/s eta 0:00:02\n",
      "     ------------ --------------------------- 4.0/12.8 MB 9.4 MB/s eta 0:00:01\n",
      "     -------------- ------------------------- 4.5/12.8 MB 9.6 MB/s eta 0:00:01\n",
      "     --------------- ------------------------ 5.1/12.8 MB 9.8 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 5.6/12.8 MB 10.0 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 6.2/12.8 MB 10.1 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 6.7/12.8 MB 10.3 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 7.3/12.8 MB 10.3 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 7.8/12.8 MB 10.4 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 8.4/12.8 MB 10.5 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 8.9/12.8 MB 10.6 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 9.5/12.8 MB 10.6 MB/s eta 0:00:01\n",
      "     ------------------------------ -------- 10.1/12.8 MB 10.7 MB/s eta 0:00:01\n",
      "     -------------------------------- ------ 10.6/12.8 MB 11.7 MB/s eta 0:00:01\n",
      "     -------------------------------- ------ 10.8/12.8 MB 11.9 MB/s eta 0:00:01\n",
      "     -------------------------------- ------ 10.8/12.8 MB 11.9 MB/s eta 0:00:01\n",
      "     --------------------------------- ----- 11.1/12.8 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------------ -- 12.1/12.8 MB 11.1 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 11.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.7.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (63.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.7.1\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3d434d5e-09bf-4732-bd0d-f7f69973be95",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0136bb7a-3257-439f-9983-466ae093e806",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(example_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "953d8a61-aee4-4e11-b52f-4e38f4ac7da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muad\n",
      "Dib\n",
      "learned\n",
      "rapidly\n",
      "because\n",
      "his\n",
      "first\n",
      "training\n",
      "was\n",
      "in\n",
      "how\n",
      "to\n",
      "learn\n",
      ".\n",
      "And\n",
      "the\n",
      "first\n",
      "lesson\n",
      "of\n",
      "all\n",
      "was\n",
      "the\n",
      "basic\n",
      "trust\n",
      "that\n",
      "he\n",
      "could\n",
      "learn\n",
      ".\n",
      "It\n",
      "is\n",
      "shocking\n",
      "to\n",
      "find\n",
      "how\n",
      "many\n",
      "people\n",
      "do\n",
      "not\n",
      "believe\n",
      "they\n",
      "can\n",
      "learn\n",
      ",\n",
      "and\n",
      "how\n",
      "many\n",
      "more\n",
      "believe\n",
      "learning\n",
      "to\n",
      "be\n",
      "difficult\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for token in doc1:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6c1d4214-452a-437c-bdc5-64133a19fb75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "5\n",
      "km\n",
      "ride\n",
      "cost\n",
      "$\n",
      "10.04\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp('A 5km ride cost $10.04')\n",
    "for token in doc2:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3f4eff2b-5054-488c-b88a-32f0eea67503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', '5km', 'ride', 'cost', '$', '10.04']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize('A 5km ride cost $10.04')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70829df-c1bb-470f-bdd2-3308b732a1b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed6f76af-fd1d-4a7e-a1b1-944bb37f4840",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c94211-cd43-453d-891f-73aaeded3736",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the words “helping” and “helper” share the root “help.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8f7509c0-5f6d-4c45-b43c-39af5a7df59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "572d0acd-e555-4869-91e7-2836a022a77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ebe3eddc-3a02-4c87-97fb-966ec087e8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_for_stemming = \"\"\"\n",
    "... The crew of the USS Discovery discovered many discoveries.\n",
    "... Discovering is what explorers do.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3922d7ce-7ca7-4a43-afe9-bc40a18b7c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(string_for_stemming) # 'walks, walking, walked'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "498f375f-546e-48a2-af7a-80559c11c1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_words = [stemmer.stem(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0ae2cf7e-176d-41fe-a934-5ace68024def",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'crew',\n",
       " 'of',\n",
       " 'the',\n",
       " 'uss',\n",
       " 'discoveri',\n",
       " 'discov',\n",
       " 'mani',\n",
       " 'discoveri',\n",
       " '.',\n",
       " 'discov',\n",
       " 'is',\n",
       " 'what',\n",
       " 'explor',\n",
       " 'do',\n",
       " '.']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26007e02-f1c5-4a2e-85fd-2d8930e39cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Those results look a little inconsistent. Why would 'Discovery' give you 'discoveri' when 'Discovering' gives you 'discov'?\n",
    "\n",
    "Understemming and overstemming are two ways stemming can go wrong:\n",
    "\n",
    "Understemming happens when two related words should be reduced to the same stem but aren’t. This is a false negative.\n",
    "Overstemming happens when two unrelated words are reduced to the same stem even though they shouldn’t be. This is a false positive.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bea5c9-7e3d-4b13-bdf5-85a4179efe97",
   "metadata": {},
   "source": [
    "# lemmatizing  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1ff2e8ff-a81e-459c-9182-a25758bba38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer     #lexical dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "14617bf2-e1f6-4526-afde-855c700e5af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b246b15b-85d2-4965-abd2-21c473020f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c38a9e69-ce77-414b-ac84-7db89b9a27d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'scarf'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"scarves\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a876d00d-f9b5-4723-9246-13d9060ed5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_for_lemmatizing = \"The friends of DeSoto love scarves.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1607eb43-a22d-41d0-8ebe-c74880b8f37b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'friends', 'of', 'DeSoto', 'love', 'scarves', '.']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = word_tokenize(string_for_lemmatizing)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7aa27d42-587a-4a4e-a61d-e570df8d9679",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5488618b-8c45-40f9-9746-f15ac1423044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'friend', 'of', 'DeSoto', 'love', 'scarf', '.']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7564ad09-430e-4058-9fb0-f12e146498f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer.lemmatize(\"worst\")                  #lemmatized a word that looked very different from its lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "09bbf33e-37d3-4307-aedb-df731797a73f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bad'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"worst\", pos=\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a7831c-1b49-4649-841d-c12c0e97f2b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d0dca0-09ce-4a28-85d1-0dde7829c9e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdf175f-3980-4d17-8730-f91470a12a8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8697ce7d-5b27-415a-8d3b-034419fe76d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fb9265-cfe6-453d-9b71-c5747c91a988",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b079c81b-4e0c-4582-864a-c5da96ad2f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
