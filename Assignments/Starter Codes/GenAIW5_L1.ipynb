{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsYmlUKuX1hq"
      },
      "outputs": [],
      "source": [
        "#langchain vs llama index"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain"
      ],
      "metadata": {
        "id": "ht-ml8QcX8El",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e60b293b-5dd7-42e5-c531-01d115eca697"
      },
      "execution_count": 1,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m974.6/974.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.8/321.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.4/127.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.0/145.0 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Hugging Face"
      ],
      "metadata": {
        "id": "oNTl6EBpYC09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU huggingface_hub"
      ],
      "metadata": {
        "id": "xFhkPn6MX8HM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community\n",
        "import langchain_community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8q4K5Pxr2JSF",
        "outputId": "72e542f0-301f-48fb-b6c0-98e0416e1d3d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.2.5-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.9.5)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.2.5)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.2.9)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.1.82)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.25.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (8.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.5->langchain_community) (0.2.1)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.5->langchain_community) (2.7.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.7->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.7->langchain_community) (24.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain_community) (3.10.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2024.6.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.7->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.5->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.5->langchain_community) (2.18.4)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain_community\n",
            "Successfully installed dataclasses-json-0.6.7 langchain_community-0.2.5 marshmallow-3.21.3 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from google.colab import auth\n",
        "\n",
        "auth.authenticate_user()\n",
        "\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = userdata.get('huggingface_key')"
      ],
      "metadata": {
        "id": "dKS60rk0X8J4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate, HuggingFaceHub, LLMChain\n",
        "\n",
        "# initialize HF LLM\n",
        "flan_t5 = HuggingFaceHub(\n",
        "    repo_id=\"google/flan-t5-base\",\n",
        "    model_kwargs={\"temperature\":1e-10}\n",
        ")\n",
        "\n",
        "# build prompt template for simple question-answering\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: \"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    prompt=prompt,\n",
        "    llm=flan_t5\n",
        ")\n",
        "\n",
        "question = \"who was barack obama?\"\n",
        "\n",
        "print(llm_chain.run(question))"
      ],
      "metadata": {
        "id": "VDctn3F-X8Mg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18533895-aa81-4d8f-92e7-32e1ddef1f29"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
            "  warn_deprecated(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "President of the United States\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qs = [\n",
        "    {'question': \"What is the capital of France?\"},\n",
        "    {'question': \"Who wrote 'To Kill a Mockingbird'?\"},\n",
        "    {'question': \"What is the boiling point of water in Fahrenheit?\"},\n",
        "    {'question': \"Who painted the Mona Lisa?\"},\n",
        "    {'question': \"What is the largest planet in our solar system?\"},\n",
        "    {'question': \"How many continents are there on Earth?\"},\n",
        "    {'question': \"What is the chemical symbol for gold?\"},\n",
        "    {'question': \"Who was the first President of the United States?\"}\n",
        "]\n",
        "\n",
        "res = llm_chain.generate(qs)\n",
        "res"
      ],
      "metadata": {
        "id": "1X8yr0CkX8PC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b5b0eb8-4b4f-47d9-9dec-77a8615b38af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LLMResult(generations=[[Generation(text='london')], [Generation(text='john dylan')], [Generation(text='212')], [Generation(text='franz heidegger')], [Generation(text='uranus')], [Generation(text='seven')], [Generation(text='g')], [Generation(text='john f kennedy')]], llm_output=None, run=[RunInfo(run_id=UUID('84247b49-271f-49c4-a4b5-4afab574b8c3')), RunInfo(run_id=UUID('d4484163-1ba9-4f6d-892e-75f38fb897df')), RunInfo(run_id=UUID('b8a06543-46fe-48d8-a7e7-b9e62577795b')), RunInfo(run_id=UUID('45831c7d-0493-4723-9581-613c9f16f09c')), RunInfo(run_id=UUID('553cf5ad-38ba-4b45-81fa-ffcc00364331')), RunInfo(run_id=UUID('86eb3710-e8ff-4c78-9e3b-17fc775a329b')), RunInfo(run_id=UUID('35b102e6-03f6-496e-8360-73ca9d4bf165')), RunInfo(run_id=UUID('78ac622a-5f06-4aa9-9563-4e34c626daff'))])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "multi_template = \"\"\"Answer the following questions one at a time.\n",
        "\n",
        "Questions:\n",
        "{questions}\n",
        "\n",
        "Answers:\n",
        "\"\"\"\n",
        "long_prompt = PromptTemplate(\n",
        "    template=multi_template,\n",
        "    input_variables=[\"questions\"]\n",
        ")\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    prompt=long_prompt,\n",
        "    llm=flan_t5\n",
        ")\n",
        "\n",
        "qs_str = (\n",
        "    \"What is the capital of France?\\n\" +\n",
        "    \"Who wrote 'To Kill a Mockingbird'?\\n\" +\n",
        "    \"What is the boiling point of water in Fahrenheit?\\n\" +\n",
        "    \"Who painted the Mona Lisa?\\n\" +\n",
        "    \"What is the largest planet in our solar system?\\n\" +\n",
        "    \"How many continents are there on Earth?\\n\" +\n",
        "    \"What is the chemical symbol for gold?\\n\" +\n",
        "    \"Who was the first President of the United States?\"\n",
        ")\n",
        "\n",
        "print(llm_chain.run(qs_str))"
      ],
      "metadata": {
        "id": "g3YYOA2KYMWG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00d8acf6-470b-4d67-9595-fccfdb86da7d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Franklin D. Roosevelt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#OpenAI"
      ],
      "metadata": {
        "id": "xQwZF71AYWU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install -qU openai"
      ],
      "metadata": {
        "id": "Y2RSWOeXYMYN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acdaf2c7-e494-48df-888d-82e9bf22a57a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/327.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m317.4/327.4 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.4/327.4 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "import openai\n",
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('api_key')\n",
        "openai.api_key = os.getenv('OPENAI_API_KEY')\n"
      ],
      "metadata": {
        "id": "5rluv5-OYMaz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f21ebc4f-92e5-4d0b-eef7-e1092f4286cf"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.35.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "turbo_instruct = OpenAI(model_name='gpt-3.5-turbo-instruct')"
      ],
      "metadata": {
        "id": "sMtCXfnuYMeZ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = LLMChain(\n",
        "    prompt=prompt,\n",
        "    llm=turbo_instruct\n",
        ")\n",
        "\n",
        "print(llm_chain.run(question))"
      ],
      "metadata": {
        "id": "rs0Z6uLGX8Ru",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d9c3775-4f48-43e4-fee7-c27366f24389"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Barack Obama was the 44th President of the United States, serving from 2009 to 2017. He was the first African American to hold the office. Prior to his presidency, Obama served as a United States Senator from Illinois. He is a member of the Democratic Party and is known for his efforts to improve healthcare and combat climate change.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qs = [\n",
        "    {'question': \"What is the capital of France?\"},\n",
        "    {'question': \"Who wrote 'To Kill a Mockingbird'?\"},\n",
        "    {'question': \"What is the boiling point of water in Fahrenheit?\"},\n",
        "    {'question': \"Who painted the Mona Lisa?\"},\n",
        "    {'question': \"What is the largest planet in our solar system?\"},\n",
        "    {'question': \"How many continents are there on Earth?\"},\n",
        "    {'question': \"What is the chemical symbol for gold?\"},\n",
        "    {'question': \"Who was the first President of the United States?\"}\n",
        "]\n",
        "llm_chain.generate(qs)"
      ],
      "metadata": {
        "id": "HSSTRYjGYgDz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21e4c2ee-28d6-4c19-8a75-2334071a2a19"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LLMResult(generations=[[Generation(text='The capital of France is Paris. ', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=\"\\nHarper Lee wrote 'To Kill a Mockingbird'.\", generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='212 degrees Fahrenheit', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='The Mona Lisa was painted by Leonardo da Vinci in the early 16th century.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='The largest planet in our solar system is Jupiter.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='There are seven continents on Earth: Asia, Africa, North America, South America, Antarctica, Europe, and Australia.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=' The chemical symbol for gold is Au.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=' George Washington was the first President of the United States.', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'prompt_tokens': 110, 'completion_tokens': 94, 'total_tokens': 204}, 'model_name': 'gpt-3.5-turbo-instruct'}, run=[RunInfo(run_id=UUID('17200648-7964-4dca-8d3d-7bc130c052d3')), RunInfo(run_id=UUID('42eb57b9-668b-44ec-9427-b2ccf809eca5')), RunInfo(run_id=UUID('86ca375f-1c77-46af-8078-55c84701ad4b')), RunInfo(run_id=UUID('f873e493-ecee-44b2-8426-1f1813297c0e')), RunInfo(run_id=UUID('292caab5-b4e7-4a54-bc1b-c5d11e1d3eba')), RunInfo(run_id=UUID('2be84f5b-e3da-417a-a971-f9468b0dba9d')), RunInfo(run_id=UUID('2f562818-8988-4133-b466-478ed99d3b00')), RunInfo(run_id=UUID('1ae7e076-eca1-4fae-bdb9-e75c2b8cac12'))])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qs = [\n",
        "    {'question': \"What is the capital of France?\"},\n",
        "    {'question': \"Who wrote 'To Kill a Mockingbird'?\"},\n",
        "    {'question': \"What is the boiling point of water in Fahrenheit?\"},\n",
        "    {'question': \"Who painted the Mona Lisa?\"},\n",
        "    {'question': \"What is the largest planet in our solar system?\"},\n",
        "    {'question': \"How many continents are there on Earth?\"},\n",
        "    {'question': \"What is the chemical symbol for gold?\"},\n",
        "    {'question': \"Who was the first President of the United States?\"}\n",
        "]\n",
        "print(llm_chain.run(qs))"
      ],
      "metadata": {
        "id": "AAamXPECYgGa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e1f98ef-eae4-40d3-938a-2c75ccf3d5db"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "1. Paris\n",
            "2. Harper Lee\n",
            "3. 212 degrees\n",
            "4. Leonardo da Vinci\n",
            "5. Jupiter\n",
            "6. Seven\n",
            "7. Au\n",
            "8. George Washington\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "multi_template = \"\"\"Answer the following questions one at a time.\n",
        "\n",
        "Questions:\n",
        "{questions}\n",
        "\n",
        "Answers:\n",
        "\"\"\"\n",
        "long_prompt = PromptTemplate(\n",
        "    template=multi_template,\n",
        "    input_variables=[\"questions\"]\n",
        ")\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    prompt=long_prompt,\n",
        "    llm=davinci\n",
        ")\n",
        "\n",
        "qs_str = (\n",
        "    \"What is the capital of France?\\n\" +\n",
        "    \"Who wrote 'To Kill a Mockingbird'?\\n\" +\n",
        "    \"What is the boiling point of water in Fahrenheit?\\n\" +\n",
        "    \"Who painted the Mona Lisa?\\n\" +\n",
        "    \"What is the largest planet in our solar system?\\n\" +\n",
        "    \"How many continents are there on Earth?\\n\" +\n",
        "    \"What is the chemical symbol for gold?\\n\" +\n",
        "    \"Who was the first President of the United States?\"\n",
        ")\n",
        "\n",
        "print(llm_chain.run(qs_str))"
      ],
      "metadata": {
        "id": "dT-HIPluYgJN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90e0b48b-8785-4a5d-dbdf-8f9051619ca9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "1. The capital of France is Paris.\n",
            "2. 'To Kill a Mockingbird' was written by Harper Lee.\n",
            "3. The boiling point of water in Fahrenheit is 212 degrees.\n",
            "4. The Mona Lisa was painted by Leonardo da Vinci.\n",
            "5. The largest planet in our solar system is Jupiter.\n",
            "6. There are seven continents on Earth.\n",
            "7. The chemical symbol for gold is Au.\n",
            "8. The first President of the United States was George Washington.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import FewShotPromptTemplate"
      ],
      "metadata": {
        "id": "yu9aWdkVYvCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example = [\n",
        "    {\n",
        "        \"query\": \"What is the capital of France?\",\n",
        "        \"answer\": \"Paris\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Who wrote 'To Kill a Mockingbird'?\",\n",
        "        \"answer\": \"Harper Lee\"\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "31DddTWLsf2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YhX2jAedsf49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N5eGHG1msf8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#llama Index"
      ],
      "metadata": {
        "id": "rtDcee61Ywy-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index-llms-openai"
      ],
      "metadata": {
        "id": "lAT9jeqAYvEr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "064136f3-c275-4be8-e241-bb48bb96687d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index-llms-openai\n",
            "  Downloading llama_index_llms_openai-0.1.23-py3-none-any.whl (11 kB)\n",
            "Collecting llama-index-core<0.11.0,>=0.10.24 (from llama-index-llms-openai)\n",
            "  Downloading llama_index_core-0.10.50-py3-none-any.whl (15.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.4/15.4 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (0.6.7)\n",
            "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (2023.6.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (0.27.0)\n",
            "Collecting llama-cloud<0.0.7,>=0.0.6 (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai)\n",
            "  Downloading llama_cloud-0.0.6-py3-none-any.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.8/130.8 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (3.3)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (3.8.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.25.2)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.35.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (2.0.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (9.4.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (2.31.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (8.4.1)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.14.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (4.0.3)\n",
            "Requirement already satisfied: pydantic>=1.10 in /usr/local/lib/python3.10/dist-packages (from llama-cloud<0.0.7,>=0.0.6->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (2.7.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (2024.6.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (0.14.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (2024.5.15)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (3.21.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (2024.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.2.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (24.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llama-cloud<0.0.7,>=0.0.6->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llama-cloud<0.0.7,>=0.0.6->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (2.18.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.16.0)\n",
            "Installing collected packages: dirtyjson, deprecated, tiktoken, llama-cloud, llama-index-core, llama-index-llms-openai\n",
            "Successfully installed deprecated-1.2.14 dirtyjson-1.0.8 llama-cloud-0.0.6 llama-index-core-0.10.50 llama-index-llms-openai-0.1.23 tiktoken-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "# non-streaming\n",
        "resp = OpenAI().complete(\"Paul Graham is \")\n",
        "print(resp)"
      ],
      "metadata": {
        "id": "FWZkWZnrYvGF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8c53aa3-61e2-46fd-a23f-60c29eaff0d6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a computer scientist, entrepreneur, and venture capitalist. He is best known for co-founding the startup accelerator Y Combinator and for his work on programming languages and web development. Graham has also written several influential essays on technology, startups, and entrepreneurship.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "# non-streaming\n",
        "completion = OpenAI().complete(\"Paul Graham is \")\n",
        "print(completion)\n",
        "\n",
        "# using streaming endpoint\n",
        "\n",
        "llm = OpenAI()\n",
        "completions = llm.stream_complete(\"Paul Graham is \")\n",
        "for completion in completions:\n",
        "    print(completion.delta, end=\"\")"
      ],
      "metadata": {
        "id": "8LSKwv_jYvdg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca6a17e7-e9b4-47ab-d08d-2d3fd5ec67b5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a computer scientist, entrepreneur, and venture capitalist. He is best known for co-founding the startup accelerator Y Combinator and for his work on Lisp, a programming language. Graham has also written several influential essays on startups, technology, and entrepreneurship.\n",
            "a computer scientist, entrepreneur, and venture capitalist. He is best known for co-founding the startup accelerator Y Combinator and for his work as an essayist and author on topics related to technology, startups, and entrepreneurship. Graham has been influential in the tech industry and is considered a thought leader in the startup community."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.llms import ChatMessage\n",
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "messages = [\n",
        "    ChatMessage(\n",
        "        role=\"system\", content=\"You are a pirate with a colorful personality\"\n",
        "    ),\n",
        "    ChatMessage(role=\"user\", content=\"What is your name\"),\n",
        "]\n",
        "resp = OpenAI().chat(messages)\n",
        "print(resp)"
      ],
      "metadata": {
        "id": "hPM8MI3oYvgE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43a4f20c-c267-4770-d580-737290633fab"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "assistant: Ahoy matey! The name's Captain Rainbowbeard! Aye, I be a pirate with a love for all things colorful and bright. Me beard be as vibrant as a rainbow, and me ship be the most colorful vessel on the seven seas! What can I do for ye today, me hearty?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo\")"
      ],
      "metadata": {
        "id": "BD4eaOGnYvil"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resp = llm.complete(\"Paul Graham is \")"
      ],
      "metadata": {
        "id": "cFUZfSfGYvk7"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(resp)"
      ],
      "metadata": {
        "id": "Bd04v0YHYvnc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c307405-de87-40ce-cef2-0a3111316de9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a computer scientist, entrepreneur, and venture capitalist. He is best known for co-founding the startup accelerator Y Combinator and for his work on programming languages and web development. Graham is also a prolific writer and has published several influential essays on technology, startups, and entrepreneurship.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    ChatMessage(\n",
        "        role=\"system\", content=\"You are a pirate with a colorful personality\"\n",
        "    ),\n",
        "    ChatMessage(role=\"user\", content=\"What is your name\"),\n",
        "]\n",
        "resp = llm.chat(messages)"
      ],
      "metadata": {
        "id": "ave6b4pfYvpg"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resp"
      ],
      "metadata": {
        "id": "gbcx18YKYvtA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b47f188-3e05-4952-9744-4273af213f7e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatResponse(message=ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='Ahoy matey! Ye can call me Captain Rainbowbeard! Aye, me beard be as colorful as a rainbow, just like me personality! Arrr!', additional_kwargs={}), raw={'id': 'chatcmpl-9dtj6lps1bjo34O2VUd7FxkimNXEg', 'choices': [Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Ahoy matey! Ye can call me Captain Rainbowbeard! Aye, me beard be as colorful as a rainbow, just like me personality! Arrr!', role='assistant', function_call=None, tool_calls=None))], 'created': 1719296032, 'model': 'gpt-3.5-turbo-0125', 'object': 'chat.completion', 'service_tier': None, 'system_fingerprint': None, 'usage': CompletionUsage(completion_tokens=34, prompt_tokens=23, total_tokens=57)}, delta=None, logprobs=None, additional_kwargs={})"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9EgbdOirYgMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index pypdf sentence_transformers -q\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwVcrt-iKQQp",
        "outputId": "5378c616-0fab-4652-88fe-a6d79d566f84"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "\n",
        "# Load documents from a directory\n",
        "documents = SimpleDirectoryReader(\"/content/Untitled Folder\").load_data()\n",
        "\n",
        "# Create an index from the documents\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "\n",
        "# Create a query engine from the index\n",
        "query_engine = index.as_query_engine()\n",
        "\n",
        "# Query the engine\n",
        "response = query_engine.query(\"What is this book about?\")\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpFX3LnZKQTR",
        "outputId": "7162dda6-4a83-476f-aa42-de34b2dd9af7"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The book contains copyright information, a notice about the evolving nature of medicine, terms of use regarding the copyrighted work, and disclaimers about the accuracy and completeness of the information provided.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqVpCT7QKQY6",
        "outputId": "d307b0af-661b-49e1-8eed-875baa90adc4"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<llama_index.core.indices.vector_store.base.VectorStoreIndex at 0x784d8338be80>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PWKzRGrqKQby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GUunsNccKQep"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}