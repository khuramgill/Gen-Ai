{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khuramgill/Gen-Ai/blob/main/OpenAI_LLamaIndex_LangChain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsYmlUKuX1hq"
      },
      "outputs": [],
      "source": [
        "#langchain vs llama index"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain"
      ],
      "metadata": {
        "id": "ht-ml8QcX8El"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Hugging Face"
      ],
      "metadata": {
        "id": "oNTl6EBpYC09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU huggingface_hub"
      ],
      "metadata": {
        "id": "xFhkPn6MX8HM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community\n",
        "import langchain_community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8q4K5Pxr2JSF",
        "outputId": "bb19c8c9-5aba-4c47-f441-5c5a6f680758"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.10/dist-packages (0.2.5)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.2.5)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.2.9)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.1.82)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.25.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (8.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.21.3)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.5->langchain_community) (0.2.1)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.5->langchain_community) (2.7.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.7->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.7->langchain_community) (24.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain_community) (3.10.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2024.6.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.7->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.5->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.5->langchain_community) (2.18.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from google.colab import auth\n",
        "\n",
        "auth.authenticate_user()\n",
        "\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = userdata.get('huggingface_key')"
      ],
      "metadata": {
        "id": "dKS60rk0X8J4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate, HuggingFaceHub, LLMChain\n",
        "\n",
        "# initialize HF LLM\n",
        "flan_t5 = HuggingFaceHub(\n",
        "    repo_id=\"google/flan-t5-base\",\n",
        "    model_kwargs={\"temperature\":1e-10}\n",
        ")\n",
        "\n",
        "# build prompt template for simple question-answering\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: \"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    prompt=prompt,\n",
        "    llm=flan_t5\n",
        ")\n",
        "\n",
        "question = \"who was barack obama?\"\n",
        "\n",
        "print(llm_chain.run(question))"
      ],
      "metadata": {
        "id": "VDctn3F-X8Mg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c2877ed-ef96-42b5-f80e-93981b262634"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "President of the United States\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qs = [\n",
        "    {'question': \"What is the capital of France?\"},\n",
        "    {'question': \"Who wrote 'To Kill a Mockingbird'?\"},\n",
        "    {'question': \"What is the boiling point of water in Fahrenheit?\"},\n",
        "    {'question': \"Who painted the Mona Lisa?\"},\n",
        "    {'question': \"What is the largest planet in our solar system?\"},\n",
        "    {'question': \"How many continents are there on Earth?\"},\n",
        "    {'question': \"What is the chemical symbol for gold?\"},\n",
        "    {'question': \"Who was the first President of the United States?\"}\n",
        "]\n",
        "\n",
        "res = llm_chain.generate(qs)\n",
        "res"
      ],
      "metadata": {
        "id": "1X8yr0CkX8PC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9444ceb8-67ea-405f-957a-a01f0b865975"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LLMResult(generations=[[Generation(text='london')], [Generation(text='john dylan')], [Generation(text='212')], [Generation(text='franz heidegger')], [Generation(text='uranus')], [Generation(text='seven')], [Generation(text='g')], [Generation(text='john f kennedy')]], llm_output=None, run=[RunInfo(run_id=UUID('d7630f78-e31f-473f-b366-fb72b9f5a34a')), RunInfo(run_id=UUID('9eb0886f-8629-446f-997d-52da9e196dff')), RunInfo(run_id=UUID('9be599e4-426b-473c-97dc-2d9c74dfc763')), RunInfo(run_id=UUID('98f09202-3f1c-4829-b414-b09f339a4656')), RunInfo(run_id=UUID('be53debd-8803-49c7-8e14-5bc2e54973a8')), RunInfo(run_id=UUID('f9a9657b-8b7a-4794-ac8b-087858f12bf8')), RunInfo(run_id=UUID('4b3d3fc1-fa83-45d1-aeae-7743890b80bc')), RunInfo(run_id=UUID('f1994a3d-1053-4b37-a78e-2249e4e66615'))])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "multi_template = \"\"\"Answer the following questions one at a time.\n",
        "\n",
        "Questions:\n",
        "{questions}\n",
        "\n",
        "Answers:\n",
        "\"\"\"\n",
        "long_prompt = PromptTemplate(\n",
        "    template=multi_template,\n",
        "    input_variables=[\"questions\"]\n",
        ")\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    prompt=long_prompt,\n",
        "    llm=flan_t5\n",
        ")\n",
        "\n",
        "qs_str = (\n",
        "    \"What is the capital of France?\\n\" +\n",
        "    \"Who wrote 'To Kill a Mockingbird'?\\n\" +\n",
        "    \"What is the boiling point of water in Fahrenheit?\\n\" +\n",
        "    \"Who painted the Mona Lisa?\\n\" +\n",
        "    \"What is the largest planet in our solar system?\\n\" +\n",
        "    \"How many continents are there on Earth?\\n\" +\n",
        "    \"What is the chemical symbol for gold?\\n\" +\n",
        "    \"Who was the first President of the United States?\"\n",
        ")\n",
        "\n",
        "print(llm_chain.run(qs_str))"
      ],
      "metadata": {
        "id": "g3YYOA2KYMWG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f426f9fb-6739-486c-8972-811bd76a128e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Franklin D. Roosevelt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#OpenAI"
      ],
      "metadata": {
        "id": "xQwZF71AYWU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install -qU openai"
      ],
      "metadata": {
        "id": "Y2RSWOeXYMYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "import openai\n",
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('api_key')\n",
        "openai.api_key = os.getenv('OPENAI_API_KEY')\n"
      ],
      "metadata": {
        "id": "5rluv5-OYMaz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7f28b33-7d66-45fc-d373-30dd40118f2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.35.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "turbo_instruct = OpenAI(model_name='gpt-3.5-turbo-instruct')"
      ],
      "metadata": {
        "id": "sMtCXfnuYMeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = LLMChain(\n",
        "    prompt=prompt,\n",
        "    llm=turbo_instruct\n",
        ")\n",
        "\n",
        "print(llm_chain.run(question))"
      ],
      "metadata": {
        "id": "rs0Z6uLGX8Ru",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13a9518c-2471-4d3a-d9b7-f42069e452a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Barack Obama was the 44th President of the United States, serving from 2009 to 2017. He was the first African American to hold the office. Prior to becoming president, he served as a United States Senator from Illinois. During his presidency, he focused on issues such as healthcare reform, climate change, and international relations. He is also known for his charismatic speaking style and his efforts to promote diversity and inclusivity in American society.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qs = [\n",
        "    {'question': \"What is the capital of France?\"},\n",
        "    {'question': \"Who wrote 'To Kill a Mockingbird'?\"},\n",
        "    {'question': \"What is the boiling point of water in Fahrenheit?\"},\n",
        "    {'question': \"Who painted the Mona Lisa?\"},\n",
        "    {'question': \"What is the largest planet in our solar system?\"},\n",
        "    {'question': \"How many continents are there on Earth?\"},\n",
        "    {'question': \"What is the chemical symbol for gold?\"},\n",
        "    {'question': \"Who was the first President of the United States?\"}\n",
        "]\n",
        "llm_chain.generate(qs)"
      ],
      "metadata": {
        "id": "HSSTRYjGYgDz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac8fb46f-f7b2-4a74-dc22-429ecace687d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LLMResult(generations=[[Generation(text='\\nThe capital of France is Paris.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=\"Harper Lee wrote 'To Kill a Mockingbird'.\", generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='212°F (100°C)', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='The Mona Lisa was painted by Leonardo da Vinci.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=' The largest planet in our solar system is Jupiter.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=' There are seven continents on Earth: Africa, Asia, Europe, North America, South America, Australia, and Antarctica.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=' The chemical symbol for gold is Au.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='George Washington was the first President of the United States.', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'prompt_tokens': 110, 'completion_tokens': 89, 'total_tokens': 199}, 'model_name': 'gpt-3.5-turbo-instruct'}, run=[RunInfo(run_id=UUID('fe08afd5-b2a5-4d3a-a506-896c67de6c9b')), RunInfo(run_id=UUID('6749aa34-493f-4991-bd85-affa7344e805')), RunInfo(run_id=UUID('29f685a0-9aad-4afc-abe9-9a6c12cbd754')), RunInfo(run_id=UUID('c1f75af8-277c-4fde-830f-bde0f1d208d1')), RunInfo(run_id=UUID('dca773ff-4254-493e-b3f8-9ef26cecbdd5')), RunInfo(run_id=UUID('0cd04654-b875-41c5-90e4-ca927b50a5bd')), RunInfo(run_id=UUID('6d8da568-a643-4154-b304-65da56df7d77')), RunInfo(run_id=UUID('8317ba30-5e16-4466-8911-fd4152e77073'))])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qs = [\n",
        "    {'question': \"What is the capital of France?\"},\n",
        "    {'question': \"Who wrote 'To Kill a Mockingbird'?\"},\n",
        "    {'question': \"What is the boiling point of water in Fahrenheit?\"},\n",
        "    {'question': \"Who painted the Mona Lisa?\"},\n",
        "    {'question': \"What is the largest planet in our solar system?\"},\n",
        "    {'question': \"How many continents are there on Earth?\"},\n",
        "    {'question': \"What is the chemical symbol for gold?\"},\n",
        "    {'question': \"Who was the first President of the United States?\"}\n",
        "]\n",
        "print(llm_chain.run(qs))"
      ],
      "metadata": {
        "id": "AAamXPECYgGa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "966f68a4-2890-4583-c327-d071935fcfa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "1. Paris\n",
            "2. Harper Lee\n",
            "3. 212°F\n",
            "4. Leonardo da Vinci\n",
            "5. Jupiter\n",
            "6. 7\n",
            "7. Au\n",
            "8. George Washington\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "multi_template = \"\"\"Answer the following questions one at a time.\n",
        "\n",
        "Questions:\n",
        "{questions}\n",
        "\n",
        "Answers:\n",
        "\"\"\"\n",
        "long_prompt = PromptTemplate(\n",
        "    template=multi_template,\n",
        "    input_variables=[\"questions\"]\n",
        ")\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    prompt=long_prompt,\n",
        "    llm=turbo_instruct\n",
        ")\n",
        "\n",
        "qs_str = (\n",
        "    \"What is the capital of France?\\n\" +\n",
        "    \"Who wrote 'To Kill a Mockingbird'?\\n\" +\n",
        "    \"What is the boiling point of water in Fahrenheit?\\n\" +\n",
        "    \"Who painted the Mona Lisa?\\n\" +\n",
        "    \"What is the largest planet in our solar system?\\n\" +\n",
        "    \"How many continents are there on Earth?\\n\" +\n",
        "    \"What is the chemical symbol for gold?\\n\" +\n",
        "    \"Who was the first President of the United States?\"\n",
        ")\n",
        "\n",
        "print(llm_chain.run(qs_str))"
      ],
      "metadata": {
        "id": "dT-HIPluYgJN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9abc21b0-70cd-4686-a6ac-b73506ff3625"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The capital of France is Paris.\n",
            "'To Kill a Mockingbird' was written by Harper Lee.\n",
            "The boiling point of water in Fahrenheit is 212 degrees.\n",
            "The Mona Lisa was painted by Leonardo da Vinci.\n",
            "The largest planet in our solar system is Jupiter.\n",
            "There are seven continents on Earth.\n",
            "The chemical symbol for gold is Au.\n",
            "The first President of the United States was George Washington.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import FewShotPromptTemplate"
      ],
      "metadata": {
        "id": "yu9aWdkVYvCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example = [\n",
        "    {\n",
        "        \"query\": \"What is the capital of France?\",\n",
        "        \"answer\": \"Paris\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Who wrote 'To Kill a Mockingbird'?\",\n",
        "        \"answer\": \"Harper Lee\"\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "31DddTWLsf2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YhX2jAedsf49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N5eGHG1msf8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#llama Index"
      ],
      "metadata": {
        "id": "rtDcee61Ywy-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index-llms-openai"
      ],
      "metadata": {
        "id": "lAT9jeqAYvEr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20a5fcc7-a8cc-4533-b3dc-706010d2c8ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index-llms-openai\n",
            "  Downloading llama_index_llms_openai-0.1.23-py3-none-any.whl (11 kB)\n",
            "Collecting llama-index-core<0.11.0,>=0.10.24 (from llama-index-llms-openai)\n",
            "  Downloading llama_index_core-0.10.50-py3-none-any.whl (15.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.4/15.4 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (0.6.7)\n",
            "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (2023.6.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (0.27.0)\n",
            "Collecting llama-cloud<0.0.7,>=0.0.6 (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai)\n",
            "  Downloading llama_cloud-0.0.6-py3-none-any.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.8/130.8 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (3.3)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (3.8.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.25.2)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.35.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (2.0.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (9.4.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (2.31.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (8.4.1)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.14.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (4.0.3)\n",
            "Requirement already satisfied: pydantic>=1.10 in /usr/local/lib/python3.10/dist-packages (from llama-cloud<0.0.7,>=0.0.6->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (2.7.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (2024.6.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (0.14.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (2024.5.15)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (3.21.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (2024.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.2.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (24.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llama-cloud<0.0.7,>=0.0.6->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llama-cloud<0.0.7,>=0.0.6->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (2.18.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.16.0)\n",
            "Installing collected packages: dirtyjson, deprecated, tiktoken, llama-cloud, llama-index-core, llama-index-llms-openai\n",
            "Successfully installed deprecated-1.2.14 dirtyjson-1.0.8 llama-cloud-0.0.6 llama-index-core-0.10.50 llama-index-llms-openai-0.1.23 tiktoken-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "# non-streaming\n",
        "resp = OpenAI().complete(\"Paul Graham is \")\n",
        "print(resp)"
      ],
      "metadata": {
        "id": "FWZkWZnrYvGF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e647a944-fd93-4846-f987-c048e2a0701b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a computer scientist, entrepreneur, and venture capitalist. He is best known for co-founding the startup accelerator Y Combinator and for his work on programming languages and web development. Graham is also a prolific writer and has published several influential essays on technology, startups, and entrepreneurship.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "# non-streaming\n",
        "completion = OpenAI().complete(\"Paul Graham is \")\n",
        "print(completion)\n",
        "\n",
        "# using streaming endpoint\n",
        "\n",
        "llm = OpenAI()\n",
        "completions = llm.stream_complete(\"Paul Graham is \")\n",
        "for completion in completions:\n",
        "    print(completion.delta, end=\"\")"
      ],
      "metadata": {
        "id": "8LSKwv_jYvdg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b7d6a39-abef-4ce5-8ddc-fe5cd40c31cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a computer scientist, entrepreneur, and venture capitalist. He is best known for co-founding the startup accelerator Y Combinator and for his work on programming languages and web development. Graham is also a prolific writer and has published several influential essays on technology, startups, and entrepreneurship.\n",
            "a computer scientist, entrepreneur, and venture capitalist. He is best known for co-founding the startup accelerator Y Combinator and for his work in the field of programming languages and software development. Graham is also a prolific writer and has published several influential essays on topics ranging from technology and startups to philosophy and education."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.llms import ChatMessage\n",
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "messages = [\n",
        "    ChatMessage(\n",
        "        role=\"system\", content=\"You are a pirate with a colorful personality\"\n",
        "    ),\n",
        "    ChatMessage(role=\"user\", content=\"What is your name\"),\n",
        "]\n",
        "resp = OpenAI().chat(messages)\n",
        "print(resp)"
      ],
      "metadata": {
        "id": "hPM8MI3oYvgE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a689dbd-941a-466f-a886-a9ecaf5b2d01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "assistant: Ahoy matey! Ye can call me Captain Rainbowbeard! Aye, me beard be as colorful as a rainbow, and me heart be as wild as the open sea! What can I do for ye today, me hearty?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo\")"
      ],
      "metadata": {
        "id": "BD4eaOGnYvil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resp = llm.complete(\"Paul Graham is \")"
      ],
      "metadata": {
        "id": "cFUZfSfGYvk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(resp)"
      ],
      "metadata": {
        "id": "Bd04v0YHYvnc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97cdf3e3-15c6-41e8-de3c-eb86c64f46a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a computer scientist, entrepreneur, and venture capitalist. He is best known for co-founding the startup accelerator Y Combinator and for his work on programming languages and web development. Graham has also written several influential essays on technology, startups, and entrepreneurship.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    ChatMessage(\n",
        "        role=\"system\", content=\"You are a pirate with a colorful personality\"\n",
        "    ),\n",
        "    ChatMessage(role=\"user\", content=\"What is your name\"),\n",
        "]\n",
        "resp = llm.chat(messages)"
      ],
      "metadata": {
        "id": "ave6b4pfYvpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resp"
      ],
      "metadata": {
        "id": "gbcx18YKYvtA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b47f188-3e05-4952-9744-4273af213f7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatResponse(message=ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='Ahoy matey! Ye can call me Captain Rainbowbeard! Aye, me beard be as colorful as a rainbow, just like me personality! Arrr!', additional_kwargs={}), raw={'id': 'chatcmpl-9dtj6lps1bjo34O2VUd7FxkimNXEg', 'choices': [Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Ahoy matey! Ye can call me Captain Rainbowbeard! Aye, me beard be as colorful as a rainbow, just like me personality! Arrr!', role='assistant', function_call=None, tool_calls=None))], 'created': 1719296032, 'model': 'gpt-3.5-turbo-0125', 'object': 'chat.completion', 'service_tier': None, 'system_fingerprint': None, 'usage': CompletionUsage(completion_tokens=34, prompt_tokens=23, total_tokens=57)}, delta=None, logprobs=None, additional_kwargs={})"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9EgbdOirYgMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index pypdf sentence_transformers -q\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwVcrt-iKQQp",
        "outputId": "518410b3-762a-4da5-9978-ec9f76ab0e05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "pKM7WSGpG233"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "\n",
        "# Load documents from a directory\n",
        "file_path = \"/content/books/\"\n",
        "documents = SimpleDirectoryReader(file_path).load_data()\n",
        "\n",
        "# Create an index from the documents\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "\n",
        "# Create a query engine from the index\n",
        "query_engine = index.as_query_engine()\n",
        "\n",
        "# Query the engine\n",
        "response = query_engine.query(\"is there any kind of healthcare material?\")\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpFX3LnZKQTR",
        "outputId": "7583794e-1ee6-401b-f2c3-319e68f46874"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yes, there is healthcare material discussed in the provided context. The text talks about the integration of Generative Artificial Intelligence (GAI) in healthcare, its potential applications in medical imaging, disease management, risk assessment, research education, drug development, and various other areas within the healthcare industry.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqVpCT7QKQY6",
        "outputId": "689b127d-a11c-496a-c698-14d2bcc89e90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<llama_index.core.indices.vector_store.base.VectorStoreIndex at 0x7992a1f6e0e0>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\"is there any kind of healthcare material?\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "PWKzRGrqKQby",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a95f8b0-b32e-41ad-b7b4-b4b62478cb99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yes, there is healthcare material discussed in the provided context. The text talks about the integration of Generative Artificial Intelligence (GAI) in healthcare, its potential applications in medical imaging, disease management, risk assessment, research education, drug development, personalized patient treatment, medical simulation and training, clinical trial optimization, and mental health support. It also mentions how GAI models can assist healthcare professionals in making clinical decisions, improving patient outcomes, and enhancing the healthcare system through innovative technology.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\"the main idea of the book is?\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "GUunsNccKQep",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07a37fb0-0c83-44dd-a903-64324dad4ced"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The main idea of the book is centered around building companies that create new things and emphasizing the importance of thinking from first principles rather than relying on established formulas for success.\n"
          ]
        }
      ]
    }
  ]
}